import torch
import torch.utils.data
from PIL import Image
import sys
import cv2
import os
import os.path
import random
import numpy as np

if sys.version_info[0] == 2:
    import xml.etree.cElementTree as ET
else:
    import xml.etree.ElementTree as ET

import collections
from maskrcnn_benchmark.structures.bounding_box import BoxList
import albumentations as A


class DIORDataset_Meta(torch.utils.data.Dataset):
    CLASSES = ("__background__ ", 'airplane', 'airport', 'baseballfield', 'basketballcourt', 'bridge', 'chimney',
               'dam', 'Expressway-Service-area', 'Expressway-toll-station',
               'harbor', 'golffield', 'groundtrackfield', 'overpass',
               'ship', 'stadium', 'storagetank', 'tenniscourt', 'trainstation', 'vehicle', 'windmill')

    CLASSES_SPLIT1_BASE = (
        "__background__ ", 'airplane', 'airport', 'dam', 'Expressway-Service-area',
        'Expressway-toll-station', 'harbor', 'golffield', 'groundtrackfield', 'overpass', 'stadium',
        'storagetank', 'tenniscourt', 'trainstation', 'vehicle', 'windmill'
    )

    CLASSES_SPLIT2_BASE = (
        "__background__ ", 'baseballfield', 'basketballcourt', 'bridge', 'chimney', 'dam',
        'Expressway-Service-area', 'golffield', 'overpass', 'ship', 'stadium',
        'storagetank', 'tenniscourt', 'trainstation', 'vehicle', 'windmill'
    )

    CLASSES_SPLIT3_BASE = (
        "__background__ ", 'airplane', 'airport', 'baseballfield', 'basketballcourt', 'bridge',
        'chimney', 'Expressway-Service-area', 'Expressway-toll-station', 'harbor', 'groundtrackfield',
        'overpass', 'ship', 'stadium', 'trainstation', 'windmill'
    )

    CLASSES_SPLIT4_BASE = (
        "__background__ ", 'airplane', 'airport', 'baseballfield', 'basketballcourt', 'bridge',
        'chimney', 'dam', 'Expressway-toll-station', 'harbor', 'golffield',
        'groundtrackfield', 'ship', 'storagetank', 'tenniscourt', 'vehicle'
    )

    CLASSES_SPLIT5_BASE = (
        "__background__ ", 'airport', 'basketballcourt', 'bridge', 'chimney',
        'dam', 'Expressway-Service-area', 'Expressway-toll-station', 'harbor', 'golffield',
        'groundtrackfield', 'overpass', 'ship', 'stadium', 'storagetank', 'vehicle'
    )

    CLASSES_SPLIT1_NOVEL = ('baseballfield', 'basketballcourt', 'bridge', 'chimney', 'ship')
    CLASSES_SPLIT2_NOVEL = ('airplane', 'airport', 'Expressway-toll-station', 'harbor', 'groundtrackfield')
    CLASSES_SPLIT3_NOVEL = ('dam', 'golffield', 'storagetank', 'tenniscourt', 'vehicle')
    CLASSES_SPLIT4_NOVEL = ('Expressway-Service-area', 'overpass', 'stadium', 'trainstation', 'windmill')
    CLASSES_SPLIT5_NOVEL = ('airplane', 'baseballfield', 'tenniscourt', 'trainstation', 'windmill')

    def __init__(self, data_dir, split, use_difficult=False, transforms=None, toofew=True, shots=200, size=224, seed=0):

        # data_dir: "DIOR-DCNet"  ,split: "trainval_split1_base"
        self.root = data_dir
        self.image_set = split
        self.keep_difficult = use_difficult
        self.transforms = transforms

        self._annopath = os.path.join(self.root, "Annotations", "%s.xml")
        self._imgpath = os.path.join(self.root, "JPEGImages", "%s.jpg")
        self._imgsetpath = os.path.join(self.root, "ImageSets", "Main", "%s.txt")

        # æ—‹è½¬å¢å¼º
        self.rotate_transform1 = A.Rotate(limit=180, p=1)
        self.horizon_flip=A.HorizontalFlip(p=0.5)

        phase = 1
        if 'split1_base' in split:
            cls = DIORDataset_Meta.CLASSES_SPLIT1_BASE
        elif 'split2_base' in split:
            cls = DIORDataset_Meta.CLASSES_SPLIT2_BASE
        elif 'split3_base' in split:
            cls = DIORDataset_Meta.CLASSES_SPLIT3_BASE
        elif 'split4_base' in split:
            cls = DIORDataset_Meta.CLASSES_SPLIT4_BASE
        elif 'split5_base' in split:
            cls = DIORDataset_Meta.CLASSES_SPLIT5_BASE
        else:
            phase = 2
            cls = DIORDataset_Meta.CLASSES
        self.cls = cls[1:]
        self.class_to_ind = dict(zip(cls, range(len(cls))))
        self.categories = dict(zip(range(len(cls)), cls))
        self.list_root = os.path.join('/remote-home/yczhang/code/part2', 'fs_list')
        if 'base' in split:
            fname = 'dior_traindict_full.txt'
            metafile = os.path.join(self.list_root, fname)
        else:
            fname = 'dior_traindict_bbox_' + str(shots) + 'shot.txt'
            metafile = os.path.join(self.list_root, fname)
        if 'standard' in split and seed > 0:
            fname = 'dior_traindict_bbox_' + str(shots) + 'shot_seed' + str(seed) + '.txt'
            metafile = os.path.join(self.list_root, fname)
        metainds = [[]] * len(self.cls)
        with open(metafile, 'r') as f:
            metafiles = []
            for line in f.readlines():
                pair = line.rstrip().split()
                if len(pair) == 2:
                    pass
                elif len(pair) == 4:
                    pair = [pair[0] + ' ' + pair[1], pair[2] + ' ' + pair[3]]
                else:
                    raise NotImplementedError('{} not recognized'.format(pair))
                metafiles.append(pair)
            metafiles = {k: v for k, v in metafiles}
            self.metalines = [[]] * len(self.cls)
            for i, clsname in enumerate(self.cls):
                with open(metafiles[clsname], 'r') as imgf:     # æ‰“å¼€çš„æ˜¯diorlist1ä¸­ä¸åŒç±»åˆ«åœ¨ä¸åŒshotä¸‹ç”¨åˆ°çš„è®­ç»ƒå›¾ç‰‡id
                    lines = [l for l in imgf.readlines()]
                    self.metalines[i] = lines
                    if (shots > 100):   # metalinesåŒ…å«äº†è¯¥ç±»æ‰€æœ‰è®­ç»ƒå›¾åƒï¼Œä»è¯¥ç±»çš„å›¾ç‰‡idä¸­éšæœºæŠ½shotsä¸ªã€‚shots<100å°±å…¨ç”¨ï¼Ÿ
                        self.metalines[i] = random.sample(self.metalines[i], shots) # åº”è¯¥æ˜¯å¾®è°ƒæ—¶å°±ä¸é‡‡æ ·ï¼Œå…¨ç”¨ã€?

        self.ids = []   # æœ€ç»ˆæ˜¯64*shotsé•¿åº¦
        self.img_size = size
        if (phase == 2):
            for j in range(len(self.cls)):
                # ä»å„shotsæ–‡ä»¶ä¸­éšæœºå–å‡ºshots*64ä¸ªå…ƒç´ ï¼Œå¯é‡å¤?
                self.metalines[j] = np.random.choice(self.metalines[j], shots).tolist()
            for i in range(shots):  # 64å€ï¼Ÿ
                metaid = []
                for j in range(len(self.cls)):
                    metaid.append([j, self.metalines[j][i].rstrip()])   # jæ˜¯ç±»åˆ«ï¼Œiæ˜¯æ•°é‡?
                self.ids.append(metaid)
        else:   # phase 1 æ¯ç±»æŠ½shotsä¸ªæ•°æ?
            for i in range(shots):
                metaid = []
                for j in range(len(self.cls)):
                    metaid.append([j, self.metalines[j][i].rstrip()])
                self.ids.append(metaid)

    def __getitem__(self, index):
        # åŸæ¥çš„DCNetç”¨çš„
        # img_ids = self.ids[index]
        # data = []
        # for cls_id, img_id in img_ids:
        #     img = cv2.imread(img_id, cv2.IMREAD_COLOR)
        #     img = img.astype(np.float32, copy=False)
        #     img -= np.array([[[102.9801, 115.9465, 122.7717]]])
        #     height, width, _ = img.shape    # åŸå›¾å°ºå¯¸
        #     img = cv2.resize(img, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)
        #     # img = Image.open(img_id).convert("RGB")
        #
        #     mask = self.get_mask(img_id, cls_id, height, width)
        #
        #     img = torch.from_numpy(img).unsqueeze(0)
        #     mask = torch.from_numpy(mask).unsqueeze(0).unsqueeze(3)
        #     imgmask = torch.cat([img, mask], dim=3)     # åŠ äº†maskï¼Œä¸æ˜¯åˆ‡å¥½çš„é‚£ç§ï¼Œæ‰€ä»¥ç›®æ ‡å¦‚æœå°ï¼Œresizeä¹‹åå°±æ›´å°äº†ã€?
        #     imgmask = imgmask.permute(0, 3, 1, 2).contiguous()  # torch.FloatTensor
        #     data.append(imgmask)
        #
        # res = torch.cat(data, dim=0)

        # # # é‡æ–°æ„å»ºï¼Œæ¨¡ä»¿PCNN
        img_ids = self.ids[index]   # indexï¼šæ¯æ¬¡bs per GPUä¸ªintå€?
        data = []
        for cls_id, img_id in img_ids:
            img = cv2.imread(img_id, cv2.IMREAD_COLOR)
            img = img.astype(np.float32, copy=False)
            img -= np.array([[[102.9801, 115.9465, 122.7717]]])
            height, width, _ = img.shape  # åŸå›¾å°ºå¯¸

            # åˆ¶ä½œåˆ‡å¥½çš„ç›®æ ‡patch
            path = img_id.split('JPEG')[0] + 'Annotations/' + img_id.split('/')[-1].split('.jpg')[0] + '.xml'
            target = ET.parse(path).getroot()
            # img = img[:, :, ::-1]  # è°ƒæ•´å›¾ç‰‡çš„é€šé“ä¸ºååºï¼Œä¾‹å¦‚RGBä¸ºBGR
            image_patch = np.zeros((self.img_size, self.img_size), dtype=np.float32)
            mask = np.zeros((self.img_size, self.img_size), dtype=np.float32)
            mask[:, :] = 0
            mask = torch.from_numpy(mask).unsqueeze(0).unsqueeze(3)
            for obj in target.iter('object'):  # éå†ä¸ªæŸå¼ å›¾å†…æ‰€æœ‰çš„<object>å…ƒç´ ã€‚ä¹Ÿå°±æ˜¯1ä¸ªæ ‡ç­?
                name = obj.find('name').text.strip()  # è¯»å–<name>å…ƒç´ ï¼Œè·å¾—ç±»åˆ«å
                if name != self.cls[cls_id]:
                    continue
                bbox = obj.find('bndbox')  # è·å¾—è¯¥å›¾åƒçš„bboxä¿¡æ¯
                pts = ['xmin', 'ymin', 'xmax', 'ymax']
                bndbox = []
                for i, pt in enumerate(pts):
                    cur_pt = int(float(bbox.find(pt).text)) - 1  # è·å–bboxä¸­è¿™äº›é”®çš„å€¼ï¼Œ-1å¹²å˜›çš„ï¼Ÿ
                    if i % 2 == 0:  # iæ˜¯å¶æ•°ï¼Œè¯»å–åˆ°çš„æ˜¯xminï¼Œxmax
                        cur_pt = int(cur_pt)  # è¿™é‡Œä¸ºä»€ä¹ˆæ²¡æœ‰é™¤ä»¥xæ¯”ä¾‹ï¼Œåƒä¸Šé¢ä¸€æ ·ï¼Ÿå› ä¸ºè¦æ ¹æ®bboxè£å‰ªï¼?
                        bndbox.append(cur_pt)
                    elif i % 2 == 1:  # iæ˜¯å¥‡æ•°ï¼Œè¯»å–åˆ°çš„æ˜¯yminï¼Œymax
                        cur_pt = int(cur_pt)
                        bndbox.append(cur_pt)  # ç›¸å½“äºä¹Ÿæ˜¯æŒ‰ptsçš„é¡ºåºåŠ å…¥bndboxäº?
                image_patch = self.crop(img, bndbox, size=self.img_size)  # æ ¹æ®bboxåˆ‡å›¾
                break
            xd = bndbox[2] - bndbox[0]
            yd = bndbox[3] - bndbox[1]
            if xd > yd:
                ratio = yd / xd
                xd = 256
                yd = int(256 * ratio)
                yup = 128 - int(yd / 2)
                ydown = 128 + int(yd / 2)
                mask[yup:ydown, :] = 1
            else:
                ratio = xd / yd
                yd = 256
                xd = int(256 * ratio)
                xup = 128 - int(xd / 2)
                xdown = 128 + int(xd / 2)
                mask[:, xup:xdown] = 1
                # ç°åœ¨image patch å’?ä¸ªæ—‹è½¬patchå¤§å°éƒ½æ˜¯256*256
            # image_patch1 = self.rotate_transform1(image=image_patch)['image']
            image_patch1 = self.horizon_flip(image=image_patch)['image']
            image_patch = torch.from_numpy(image_patch).unsqueeze(0)
            image_patch1 = torch.from_numpy(image_patch1).unsqueeze(0)
            image_patch = torch.cat([image_patch, mask], dim=3)
            image_patch1 = torch.cat([image_patch1, mask], dim=3)
            image_patch = image_patch.permute(0, 3, 1, 2).contiguous()
            image_patch1 = image_patch1.permute(0, 3, 1, 2).contiguous()
            # data.append(image_patch)    # torch.FloatTensor [1, 4, 256, 256]
            data.append(image_patch1)   # torch.FloatTensor
        res = torch.cat(data, dim=0)    # torch.FloatTensor  res.shape=[20, 4, 256, 256]
        return res

    # æ¥è‡ªPCNN
    def crop(self, image, purpose, size):
        # è£å‰ªpurposeåŒºåŸŸå†…çš„å›¾ç‰‡å¹¶resizeåˆ°sizeï¼Œä¿æŒbboxå®½é«˜æ¯”ä¸å˜ï¼Œå…¶ä½™éƒ¨åˆ†å¡?
        # ä¸å¤ªç†è§£ä¸‹é¢è¿™äº›å˜æ¢ï¼Œå¥½åƒå¹¶æ²¡æœ‰å¾ˆå¤§çš„ä½œç”¨ï¼Ÿå˜æ¥å˜å»éƒ½æ˜¯ä¸€æ ·çš„ã€?
        # æå–å›¾ç‰‡åœ¨purposeå³bboxå†…çš„éƒ¨åˆ†
        cut_image = image[int(purpose[1]):int(purpose[3]), int(purpose[0]):int(purpose[2]), :]
        # è£å‰ªåå›¾ç‰‡çš„é«˜å®½
        height, width = cut_image.shape[0:2]
        # å®½é«˜ä¸­çš„æœ€å¤§å€?
        max_hw = max(height, width)
        # ctyï¼Œctxä¸ºä¸€åŠçš„é«˜å®½
        cty, ctx = [height // 2, width // 2]
        # ci=æ­£æ–¹å½?
        cropped_image = np.zeros((max_hw, max_hw, 3), dtype=cut_image.dtype)
        # ä¸ç®¡maxæ˜¯å®½è¿˜æ˜¯é«˜ï¼Œx0 y0éƒ½æ˜¯0ï¼Œx1 y1éƒ½æ˜¯width heightï¼Œåªæ˜¯å¯èƒ½æŠ›å¼ƒäº†ä¸€ä¸¤ä¸ªç‚¹å› ä¸ºè§¦å?
        x0, x1 = max(0, ctx - max_hw // 2), min(ctx + max_hw // 2, width)
        y0, y1 = max(0, cty - max_hw // 2), min(cty + max_hw // 2, height)
        # è‹¥x0 y0ä¸æ˜¯0 åˆ™æŠ¥å¼‚å¸¸
        assert x0 == 0
        assert y0 == 0
        # left=width//2ï¼Œright=width//2
        # top=height//2ï¼?bottom=height//2
        left, right = ctx - x0, x1 - ctx
        top, bottom = cty - y0, y1 - cty

        cropped_cty, cropped_ctx = max_hw // 2, max_hw // 2
        y_slice = slice(cropped_cty - top, cropped_cty + bottom)
        x_slice = slice(cropped_ctx - left, cropped_ctx + right)
        cropped_image[y_slice, x_slice, :] = cut_image[y0:y1, x0:x1, :]

        return cv2.resize(cropped_image, dsize=(size, size), interpolation=cv2.INTER_LINEAR)

    def get_img_info(self, index):
        cls_id, img_id = self.ids[index][0]
        path = img_id.split('JPEG')[0] + 'Annotations/' + img_id.split('/')[-1].split('.jpg')[0] + '.xml'
        anno = ET.parse(path).getroot()
        size = anno.find("size")
        im_info = tuple(map(int, (size.find("height").text, size.find("width").text)))
        return {"height": im_info[0], "width": im_info[1]}

    def __len__(self):
        return len(self.ids)

    def get_mask(self, img_id, cls_id, height, width):
        mask = np.zeros((self.img_size, self.img_size), dtype=np.float32)
        y_ration = float(height) / self.img_size
        x_ration = float(width) / self.img_size

        path = img_id.split('JPEG')[0] + 'Annotations/' + img_id.split('/')[-1].split('.jpg')[0] + '.xml'
        target = ET.parse(path).getroot()
        for obj in target.iter('object'):
            difficult = False  # int(obj.find("difficult").text) == 1
            if difficult:
                continue
            name = obj.find('name').text.strip()
            if (name != self.cls[cls_id]):
                continue
            bbox = obj.find('bndbox')
            pts = ['xmin', 'ymin', 'xmax', 'ymax']
            bndbox = []
            for i, pt in enumerate(pts):
                cur_pt = int(float(bbox.find(pt).text)) - 1
                if i % 2 == 0:
                    cur_pt = int(cur_pt / x_ration)
                    bndbox.append(cur_pt)
                elif i % 2 == 1:
                    cur_pt = int(cur_pt / y_ration)
                    bndbox.append(cur_pt)
            mask[bndbox[1]:bndbox[3], bndbox[0]:bndbox[2]] = 1
            break
        return mask

    def get_groundtruth(self, index):
        img_id = self.ids[index]
        anno = ET.parse(self._annopath % img_id).getroot()
        anno = self._preprocess_annotation(anno)

        height, width = anno["im_info"]
        target = BoxList(anno["boxes"], (width, height), mode="xyxy")
        target.add_field("labels", anno["labels"])
        # target.add_field("difficult", anno["difficult"])
        return target

    def _preprocess_annotation(self, target):
        boxes = []
        gt_classes = []
        difficult_boxes = []
        TO_REMOVE = 1

        for obj in target.iter("object"):
            difficult = False  # int(obj.find("difficult").text) == 1
            if not self.keep_difficult and difficult:
                continue
            name = obj.find("name").text.lower().strip()
            bb = obj.find("bndbox")
            # Make pixel indexes 0-based
            # Refer to "https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/datasets/pascal_voc.py#L208-L211"
            box = [
                bb.find("xmin").text,
                bb.find("ymin").text,
                bb.find("xmax").text,
                bb.find("ymax").text,
            ]
            bndbox = tuple(
                map(lambda x: x - TO_REMOVE, list(map(int, box)))
            )

            boxes.append(bndbox)
            gt_classes.append(self.class_to_ind[name])
            difficult_boxes.append(difficult)

        size = target.find("size")
        im_info = tuple(map(int, (size.find("height").text, size.find("width").text)))

        res = {
            "boxes": torch.tensor(boxes, dtype=torch.float32),
            "labels": torch.tensor(gt_classes),
            "difficult": torch.tensor(difficult_boxes),
            "im_info": im_info,
        }
        return res

    def map_class_id_to_class_name(self, class_id):
        # return PascalVOCDataset.CLASSES[class_id]
        return self.cls[class_id]
